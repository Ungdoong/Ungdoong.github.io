# 인공지능4



## Deep Natural Language Processing

### - Word, Sentence, and Document Embedding

____________________________

## Word Embedding

###  one-hot encoding

​	각 단어들의 빈도수에 따라 테이블 생성

​	단어간의 관계를 예측할 수 없음

### Continuous(dense) Vector Representations

​	단어들을 벡터로 표현

​	비슷한 의미를 가진 단어들은 가까이 있음

​	ex) Word2Vec

### Distributed Representations

- Many-to-many relationship
  - a concept - many neurons
  - a neuron - many concepts

### Word2Vec

- CBOW

  중간 생략된 단어 예측

- Skip-Gram

  중간 단어를 활용해 앞 뒤 문맥을 예측

### Evaluation

- Word Similarity Task

  각 단어 사이의 유사성을 **Human Evaluation**와 **Cosine Similarity**로 표현

- Word Analogy Task

  단어 사이의 관계를 통해 제시된 문제를 예측

  ex) Man : Woman = King : ?

#### 단어 수준 접근에서의 문제점

- 비사전적 단어들
- 희박한 단어들



### Subword Information Skip-Gram ( a.k.a FastText )

​	단어를 쪼개어 모델링

​	ex) where → wh, he ...



### Contextualized Word Embedding

​	같은 단어라도 문맥에 따라 Embedding이 달라짐